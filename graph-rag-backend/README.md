# Graph RAG Backend - Production-Ready Platform

A comprehensive, extensible Graph RAG (Retrieval-Augmented Generation) platform with agentic reasoning, multi-modal retrieval, and pluggable graph database support.

## ğŸŒŸ Features

### Core Capabilities
- **Automatic Knowledge Graph Construction**: LLM-powered ontology extraction from documents
- **Multi-Modal Retrieval**: Combines vector search, graph traversal, and logical filtering
- **Agentic Orchestration**: AI agents dynamically determine optimal retrieval strategies
- **Entity Resolution**: Automatic deduplication and entity merging
- **Pluggable Architecture**: Support for Neo4j, AWS Neptune, and in-memory graphs
- **Real-time Processing**: Streaming responses with reasoning chain visibility

### System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents  â”‚â”€â”€â”€â”€â–¶â”‚   Pipeline   â”‚â”€â”€â”€â”€â–¶â”‚   Neo4j     â”‚
â”‚  (PDF/DOCX) â”‚     â”‚  Processing  â”‚     â”‚   Graph     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                     â”‚
                            â–¼                     â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Embeddings  â”‚     â”‚   Agentic   â”‚
                    â”‚    (FAISS)   â”‚â—€â”€â”€â”€â–¶â”‚  Retrieval  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Option 1: Docker Compose (Recommended)

```bash
# Clone the repository
git clone <your-repo-url>
cd graph-rag-backend

# Start all services
docker-compose up -d

# Pull Ollama model
docker exec -it graphrag-ollama ollama pull llama2

# Check status
docker-compose ps
```

Services will be available at:
- API: http://localhost:8000
- Neo4j Browser: http://localhost:7474
- API Docs: http://localhost:8000/docs

### Option 2: Local Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up Neo4j (optional - will use in-memory if unavailable)
# Download from: https://neo4j.com/download/

# Set up Ollama (optional - for LLM features)
# Download from: https://ollama.ai/
ollama pull llama2

# Create .env file
cp .env.example .env

# Run the application
uvicorn main:app --reload
```

## ğŸ“ Project Structure

```
graph-rag-backend/
â”œâ”€â”€ main.py                      # FastAPI application
â”œâ”€â”€ config.py                    # Configuration management
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ docker-compose.yml           # Docker orchestration
â”œâ”€â”€ Dockerfile                   # Container definition
â”œâ”€â”€ .env.example                 # Environment variables template
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor.py   # Document ingestion pipeline
â”‚   â”œâ”€â”€ graph_constructor.py    # Graph building logic
â”‚   â”œâ”€â”€ graph_service.py        # Graph database interface
â”‚   â”œâ”€â”€ entity_resolver.py      # Entity deduplication
â”‚   â”œâ”€â”€ ontology_manager.py     # Ontology management
â”‚   â”œâ”€â”€ agentic_retrieval.py    # Multi-agent retrieval
â”‚   â””â”€â”€ metrics_collector.py    # Performance metrics
â””â”€â”€ tests/
    â”œâ”€â”€ test_api.py
    â”œâ”€â”€ test_graph.py
    â””â”€â”€ test_retrieval.py
```

## ğŸ”§ Configuration

Create a `.env` file:

```env
# Graph Database
GRAPH_DB_TYPE=neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password

# LLM Provider
LLM_PROVIDER=ollama
OLLAMA_ENDPOINT=http://localhost:11434/api/generate
OLLAMA_MODEL=llama2

# Optional: OpenAI
OPENAI_API_KEY=your_key_here

# Embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Processing
CHUNK_SIZE=500
ENTITY_SIMILARITY_THRESHOLD=0.85
```

## ğŸ“š API Usage

### Upload Document
```bash
curl -X POST "http://localhost:8000/api/documents/upload" \
  -F "file=@document.pdf"
```

### Query Knowledge Base
```bash
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Who manages the Marketing department in Delhi?",
    "max_results": 10,
    "use_vector": true,
    "use_graph": true,
    "use_filter": true
  }'
```

### Get Graph Statistics
```bash
curl "http://localhost:8000/api/graph/stats"
```

### View Ontology
```bash
curl "http://localhost:8000/api/graph/ontology"
```

## ğŸ§ª Testing

```bash
# Install test dependencies
pip install pytest pytest-asyncio httpx

# Run tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=services --cov-report=html
```

## ğŸ—ï¸ Architecture Details

### Document Processing Pipeline

1. **Text Extraction**: Supports PDF, DOCX, TXT formats
2. **Chunking**: Sentence-aware text splitting
3. **Ontology Extraction**: LLM-powered entity and relationship identification
4. **Embedding Generation**: Vector representations using sentence-transformers
5. **Graph Construction**: Neo4j graph building with properties
6. **Entity Resolution**: Similarity-based deduplication

### Agentic Retrieval System

The system uses multiple specialized agents:

- **Coordinator Agent**: Analyzes query complexity and plans strategy
- **Vector Search Agent**: Semantic similarity using embeddings
- **Graph Traversal Agent**: Relationship-based exploration
- **Filter Agent**: Metadata and attribute constraints
- **Synthesis Agent**: Merges results and generates final answer

### Graph Database Interface

Unified interface supporting:
- **Neo4j**: Production graph database with Cypher queries
- **AWS Neptune**: Cloud-native graph database with Gremlin
- **In-Memory**: Testing without external dependencies

## ğŸ”Œ Integration Examples

### Python SDK
```python
import requests

# Upload document
with open('document.pdf', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/api/documents/upload',
        files={'file': f}
    )
    doc_id = response.json()['document_id']

# Query
response = requests.post(
    'http://localhost:8000/api/query',
    json={'query': 'What are the main projects?'}
)
answer = response.json()['answer']
```

### Frontend Integration
```javascript
// Upload document
const formData = new FormData();
formData.append('file', file);

const uploadResponse = await fetch('http://localhost:8000/api/documents/upload', {
  method: 'POST',
  body: formData
});

// Query with streaming
const response = await fetch('http://localhost:8000/api/query/stream', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ query: 'Your question here' })
});

const reader = response.body.getReader();
// Process streaming response
```

## ğŸ¯ Performance Optimization

### For Production:

1. **Use Neo4j Enterprise** for better performance
2. **Enable GPU** for Ollama (faster LLM inference)
3. **Scale with Redis** for caching
4. **Use load balancer** for multiple API instances
5. **Optimize embeddings** with quantization

### Benchmarks:
- Document processing: ~2-5 seconds per document
- Query latency: ~300-800ms (depends on graph size)
- Concurrent requests: 100+ with proper resources

## ğŸ› Troubleshooting

### Neo4j Connection Issues
```bash
# Check Neo4j status
docker logs graphrag-neo4j

# Verify connection
curl http://localhost:7474
```

### Ollama Not Responding
```bash
# Check Ollama status
docker exec -it graphrag-ollama ollama list

# Pull model if missing
docker exec -it graphrag-ollama ollama pull llama2
```

### Memory Issues
```bash
# Increase Docker memory limit
# Docker Desktop -> Settings -> Resources -> Memory

# Or use in-memory mode
export GRAPH_DB_TYPE=memory
```

## ğŸ“Š Monitoring

Access metrics at: `http://localhost:8000/api/admin/metrics`

```json
{
  "queries": {
    "total": 150,
    "successful": 148,
    "avg_latency_ms": 425.5
  },
  "documents": {
    "total_processed": 25,
    "total_entities": 1250
  },
  "graph": {
    "total_nodes": 1250,
    "total_edges": 3420
  }
}
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ†˜ Support

- Documentation: [API Docs](http://localhost:8000/docs)
- Issues: GitHub Issues
- Email: support@example.com

## ğŸ—ºï¸ Roadmap

- [ ] Multi-tenancy support
- [ ] Advanced entity linking (Wikidata, DBpedia)
- [ ] Real-time collaboration features
- [ ] Graph visualization UI improvements
- [ ] Support for more LLM providers
- [ ] Automatic schema evolution
- [ ] Query optimization with caching
- [ ] Multi-language support